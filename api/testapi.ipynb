{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unique_ids': [1, 2]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:5000/remove_duplicates?day_range=1\"\n",
    "\n",
    "data = {\n",
    "    \"Message\": [\"Привіт\", \"Як справи?\", \"Привіт\"],\n",
    "    \"MessageDate\": [\"2029-01-28 19:28:10.123\", \"2029-01-29 10:00:00.000\", \"2029-01-28 19:28:10.123\"],\n",
    "    \"TelegramPostInfoID\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'message': 'Повідомлення успішно отримані та збережені'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "url = \"http://127.0.0.1:5001/api/fetch_posts\"  \n",
    "payload = {\n",
    "    \"channel\": \"news_channel\",\n",
    "    \"start_date\": \"2024-02-01\",\n",
    "    \"end_date\": \"2024-02-05\",\n",
    "    \"model\": \"Bert\"  \n",
    "}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/backend/database/database.db\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/backend/database/database.db\"\n",
    "print(DB_PATH)\n",
    "import sqlite3\n",
    "def get_db_connection():\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)  \n",
    "    conn.row_factory = sqlite3.Row \n",
    "    return conn\n",
    "\n",
    "\n",
    "def fetch_table_contents(table_name):\n",
    "\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        if rows:\n",
    "\n",
    "            for row in rows:\n",
    "                print(dict(row)) \n",
    "        else:\n",
    "            print(f\" {table_name} empty.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "def show_all_tables():\n",
    "    fetch_table_contents(\"TelegramPostInfo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MessageID': 8421, 'Message': 'Ка-52М в составе смешанной тактической группы уничтожил бронетехнику и личный состав ВСУ в приграничном районе Курской области.\\n\\n#вертовойна', 'Channel': '@vertolatte', 'MessageDate': '2025-01-05 15:00:28.000', 'Name': '', 'Location': 'курской областить', 'Weapons': 'ка-52', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8422, 'Message': '⚡️Очередная колонна \"отконтрнаступалась\"\\n\\n#Бесстрашные \\n\\nНа подступах к н.п. Бердин Курской области экипаж вертолёта Ка-52М  армейской авиации ВС РФ нанёс удар по колонне бронетехники ВСУ во время высадки украинских фашистов.\\n\\nСеверный Ветер', 'Channel': '@vertolatte', 'MessageDate': '2025-01-05 22:05:37.000', 'Name': '', 'Location': 'рф, курской областить', 'Weapons': 'ка-52', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8424, 'Message': '6 января 2001\\xa0года состоялось первое боевое применение вертолета Ка-50 «Черная акула» в Чечне, в составе БУГ (боевой ударной группы, состоящей из двух Ка-50 и одного Ка-29ВПНЦУ).', 'Channel': '@vertolatte', 'MessageDate': '2025-01-08 15:29:16.000', 'Name': '', 'Location': 'чечня', 'Weapons': '', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8429, 'Message': 'В акватории залива Петра Великого на большом противолодочном корабле Тихоокеанского флота (ТОФ) «Адмирал Трибуц» проведена тренировка по приёму на борт вертолёта морской авиации Ка-27ПС. Об этом сообщила пресс-служба Минобороны. \\n \\nЭкипаж вертолёта совершил перелёт с аэродрома базирования в Приморском крае,. На подлёте к району нахождения БПК вертолётчики провели тренировку по взаимодействию с воздушным командным пунктом управления боевого корабля. Военнослужащие авиационной группы на противолодочном корабле выполнили комплекс мероприятий по обеспечению посадки вертолётов на палубу.\\n\\n#вертофлот', 'Channel': '@vertolatte', 'MessageDate': '2025-01-14 08:47:55.000', 'Name': '', 'Location': 'петра великий, приморском край', 'Weapons': 'пк, тор, оса', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8430, 'Message': 'Музейный комплекс в Верхней Пышме в Свердловской области пополнился редким экспонатом вертолетом Ми-24А. \\n\\n\"С 1971 по 1973 годы было выпущено около 250 штук. Эту редкую машину можно встретить всего в нескольких отечественных музеях. Один из них с января 2025 года - музейный комплекс в Верхней Пышме\", - говорится в сообщении музея.\\n\\nПилотская кабина Ми-24А имела характерное остекление, за  которое машину тут же окрестили \"верандой\" или \"стаканом\" за сходство с граненым произведением Веры Мухиной. Причем кабина была двухместной. Пилот в ней размещался с небольшим смещением за стрелком-оператором. Такой вариант оказался не самым удачным, потому вскоре на Ми-24 появились отдельные кабины для экипажа расположенные тандемно.\\n\\n#вертомузеи', 'Channel': '@vertolatte', 'MessageDate': '2025-01-14 10:40:12.000', 'Name': 'веры мухин', 'Location': 'свердловской областить, верхней пышма', 'Weapons': 'тор', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8433, 'Message': '\"Серый волк\" и ядерная шапочка\\n\\nВоенно-воздушные силы (ВВС) США получили шестой вертолет MH-139A Grey Wolf, который задействуют в охране мест базирования межконтинентальных баллистических ракет (МБР) Minuteman III. Об этом сообщает Army Recognition. \\n \\nВертолет приписан к 341-му ракетному полку, который отвечает за эксплуатацию, техническое обслуживание и безопасность МБР Minuteman III \\n \\nВ основе «Серого волка» лежит многоцелевой вертолет Leonardo AW139. Новые машины выпускают в рамках контракта на 2,4 миллиарда долларов, заключенного в 2018 году. MH-139A заменят вертолеты UH-1N Huey, которые эксплуатируют с 1970-х годов. Grey Wolf превосходит UH-1 по скорости, дальности полета и грузоподъемности.\\n\\n#вертовойна #США', 'Channel': '@vertolatte', 'MessageDate': '2025-01-15 11:18:49.000', 'Name': '', 'Location': 'сша', 'Weapons': 'тор', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8445, 'Message': '15 января 2025 года совершил свой первый испытательный полет новый боевой вертолет Surion Marine Attack Helicopter (MAH), разрабатываемый корпорацией Korean Aerospace Industries (KAI) для нужд морской пехоты Южной Кореи.', 'Channel': '@vertolatte', 'MessageDate': '2025-01-17 13:36:21.000', 'Name': '', 'Location': 'южной корея', 'Weapons': '', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8449, 'Message': 'Холдинг \"Вертолеты России\" передал 14 вертолетов Ми-8МТВ-1 авиакомпаниям\\n\\n10 новых вертолетов Ми-8МТВ-1 группе Utair и по одному воздушному судну авиакомпаниям \"Конверс авиа\", \"Комиавиатранс\", \"Лидер\" и \"2-й Архангельский объединенный авиаотряд\", сообщили в госкорпорации \"Ростех\".\\n\\nВсего в 2024 году холдинг \"Вертолеты России\" поставил ГТЛК 40 многоцелевых Ми-8МТВ-1. Новую технику, кроме ранее упомянутых авиакомпаний, получили \"Авиация Колымы\", \"Вологодское авиапредприятие\", \"Витязь-Aэро\", \"Авиакомпания \"Ельцовка\", \"Чукотавиа\", \"Красавиа\", \"Геликс аэро\", \"Поляр авиа\", \"Водолет\", \"НПК ПАНХ\", \"АК Баргузин\".\\n\\n#вертоновости', 'Channel': '@vertolatte', 'MessageDate': '2025-01-20 14:49:48.000', 'Name': '', 'Location': '', 'Weapons': 'пк', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n",
      "{'MessageID': 8452, 'Message': 'Вертолеты Ми-24 ВСУ. Съемка ведется с довольной редкой машины Ми-24ВП с подвижной пушечной установкой ГШ-23Л.\\n\\nМи-24ВП в конце 80-х выпустили 25 штук. Модернизированная версия ВП затем стала Ми-35М. \\n\\n#вертовойна', 'Channel': '@vertolatte', 'MessageDate': '2025-01-23 13:41:05.000', 'Name': '', 'Location': '', 'Weapons': '', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n"
     ]
    }
   ],
   "source": [
    "show_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 11 fields in line 31, saw 13\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/api/ODCR-6.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 11 fields in line 31, saw 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/api/ODCR-6.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Channel': '@vertolatte', 'Message': 'В парке винтокрылых машин 2-го Архангельского объединенного авиаотряда пополнение. Совершив прямой перелёт из Казани за пять часов в Васьково прибыл новый Ми-8МТВ-1. Данный вертолет планируют использовать для перевозки пассажиров труднодоступных территорий, обеспечения санитарной авиации и доставки грузов.\\n\\n#вертоновости', 'MessageDate': '2025-01-02 14:17:49.000', 'TelegramPostInfoID': 8420}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "API_URL = \"http://127.0.0.1:5001/api/posts\"  \n",
    "\n",
    "def test_get_posts():\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_URL)  \n",
    "        if response.status_code == 200:\n",
    "            posts = response.json()\n",
    "\n",
    "            for post in posts:\n",
    "                print(post)\n",
    "        else:\n",
    "            print(f\" {response.status_code}, response: {response.text}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" {e}\")\n",
    "test_get_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeppavlov\n",
      "  Downloading deeppavlov-1.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting fastapi<=0.89.1,>=0.47.0 (from deeppavlov)\n",
      "  Downloading fastapi-0.89.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting filelock<3.10.0,>=3.0.0 (from deeppavlov)\n",
      "  Downloading filelock-3.9.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting nltk<3.10.0,>=3.2.4 (from deeppavlov)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy<1.24 (from deeppavlov)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting pandas<1.6.0,>=1.0.0 (from deeppavlov)\n",
      "  Downloading pandas-1.5.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting prometheus-client<=1.16.0,>=0.13.0 (from deeppavlov)\n",
      "  Downloading prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pydantic<2 (from deeppavlov)\n",
      "  Downloading pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl.metadata (153 kB)\n",
      "Collecting pybind11==2.10.3 (from deeppavlov)\n",
      "  Downloading pybind11-2.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.19.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from deeppavlov) (2.32.3)\n",
      "Collecting tqdm<4.65.0,>=4.42.0 (from deeppavlov)\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "Collecting uvicorn<0.19.0,>=0.13.0 (from deeppavlov)\n",
      "  Downloading uvicorn-0.18.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: wheel in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from deeppavlov) (0.45.1)\n",
      "Collecting scikit-learn<1.1.0,>=0.24 (from deeppavlov)\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (10 kB)\n",
      "Collecting scipy==1.10.0 (from deeppavlov)\n",
      "  Downloading scipy-1.10.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (53 kB)\n",
      "Collecting starlette==0.22.0 (from fastapi<=0.89.1,>=0.47.0->deeppavlov)\n",
      "  Downloading starlette-0.22.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (4.8.0)\n",
      "Requirement already satisfied: click in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2024.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pydantic<2->deeppavlov) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2024.12.14)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from scikit-learn<1.1.0,>=0.24->deeppavlov) (3.5.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from uvicorn<0.19.0,>=0.13.0->deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<1.6.0,>=1.0.0->deeppavlov) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.3.1)\n",
      "Downloading deeppavlov-1.7.0-py3-none-any.whl (492 kB)\n",
      "Downloading pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
      "Downloading scipy-1.10.0-cp310-cp310-macosx_12_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.89.1-py3-none-any.whl (55 kB)\n",
      "Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
      "Downloading filelock-3.9.1-py3-none-any.whl (9.7 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-macosx_11_0_arm64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "Downloading pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.0.2-cp310-cp310-macosx_12_0_arm64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
      "Installing collected packages: uvicorn, tqdm, pydantic, pybind11, prometheus-client, numpy, filelock, starlette, scipy, pandas, nltk, scikit-learn, fastapi, deeppavlov\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.6\n",
      "    Uninstalling pydantic-2.10.6:\n",
      "      Successfully uninstalled pydantic-2.10.6\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.2\n",
      "    Uninstalling numpy-2.2.2:\n",
      "      Successfully uninstalled numpy-2.2.2\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.17.0\n",
      "    Uninstalling filelock-3.17.0:\n",
      "      Successfully uninstalled filelock-3.17.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.1\n",
      "    Uninstalling scipy-1.15.1:\n",
      "      Successfully uninstalled scipy-1.15.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "faiss-cpu 1.9.0.post1 requires numpy<3.0,>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed deeppavlov-1.7.0 fastapi-0.89.1 filelock-3.9.1 nltk-3.9.1 numpy-1.23.5 pandas-1.5.3 prometheus-client-0.21.1 pybind11-2.10.3 pydantic-1.10.21 scikit-learn-1.0.2 scipy-1.10.0 starlette-0.22.0 tqdm-4.64.1 uvicorn-0.18.3\n"
     ]
    }
   ],
   "source": [
    "# experience.py\n",
    "#!pip install pymorphy2 pymorphy2-dicts-ru\n",
    "!pip install deeppavlov\n",
    "\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def experience_bert(text, model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return torch.argmax(outputs.logits).item()\n",
    "\n",
    "def experience_xg_boost(text, xgb_model_path=\"xgb_model.ubj\", bert_model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        transformer_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        transformer_model.to(device)\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = transformer_model(**inputs, output_hidden_states=True)\n",
    "            embeddings = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(xgb_model_path)\n",
    "        dmatrix = xgb.DMatrix(embeddings)\n",
    "        return int(xgb_model.predict(dmatrix)[0] > 0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experience_xg_boost: {e}\")\n",
    "        return 0\n",
    "\n",
    "# ner.py\n",
    "from deeppavlov import build_model, configs\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def get_ner_model():\n",
    "    return build_model(configs.ner.ner_rus_bert, download=True)\n",
    "\n",
    "def normalize_words(words):\n",
    "    morph = MorphAnalyzer()\n",
    "    return list({morph.parse(word)[0].normal_form for word in words})\n",
    "\n",
    "def get_name(mes, ner_model):\n",
    "    ner_results = ner_model([mes])\n",
    "    names = {word for word, label in zip(ner_results[0][0], ner_results[1][0]) if label in [\"B-PER\", \"I-PER\"]}\n",
    "    return normalize_words(names)\n",
    "\n",
    "def get_location(mes, ner_model):\n",
    "    ner_results = ner_model([mes])\n",
    "    locations = {word for word, label in zip(ner_results[0][0], ner_results[1][0]) if label in [\"B-LOC\", \"I-LOC\"]}\n",
    "    return normalize_words(locations)\n",
    "\n",
    "def get_weapons(mes, weapons_list_path=\"weapon.txt\"):\n",
    "    try:\n",
    "        with open(weapons_list_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            weapons = {line.strip().lower() for line in file}\n",
    "        return [w for w in weapons if w in mes.lower()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: weapons.txt not found. Returning empty list.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deeppavlov 1.7.0 requires numpy<1.24, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 01:43:07.349 INFO in 'deeppavlov.core.data.utils'['utils'] at line 97: Downloading from http://files.deeppavlov.ai/v1/ner/ner_rus_bert_torch_new.tar.gz to /Users/ulanagusar/.deeppavlov/models/ner_rus_bert_torch_new.tar.gz\n",
      "100%|██████████| 1.44G/1.44G [03:10<00:00, 7.57MB/s]\n",
      "2025-02-08 01:46:17.976 INFO in 'deeppavlov.core.data.utils'['utils'] at line 284: Extracting /Users/ulanagusar/.deeppavlov/models/ner_rus_bert_torch_new.tar.gz archive into /Users/ulanagusar/.deeppavlov/models/ner_rus_bert_torch\n",
      "/Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages/sklearn/utils/__init__.py:108: FutureWarning: IS_PYPY is deprecated and will be removed in 1.7.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_ner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_location(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mУ Дніпрі новий наступ\u001b[39m\u001b[38;5;124m\"\u001b[39m, ner_model)\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mget_ner_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_ner_model\u001b[39m():\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner_rus_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/commands/infer.py:55\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m parameter for the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m component, so \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m will not be renewed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m, component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m))))\n\u001b[0;32m---> 55\u001b[0m component \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m component_config:\n\u001b[1;32m     58\u001b[0m     model\u001b[38;5;241m.\u001b[39m_components_dict[component_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m component\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/common/params.py:92\u001b[0m, in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(e)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m---> 92\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# find the submodels params recursively\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     config_params \u001b[38;5;241m=\u001b[39m {k: _init_param(v, mode) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config_params\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/common/registry.py:74\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not registered.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_from_str(name)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcls_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_REGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/common/registry.py:42\u001b[0m, in \u001b[0;36mcls_from_str\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected class description in a `module.submodules:ClassName` form, but got `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m                       \u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m, cls_name)\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/models/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeeppavlov\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RedirectedPrints\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDP_SKIP_NLTK_DOWNLOAD\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/__init__.py:146\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/chunk/__init__.py:155\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamed_entity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/chunk/api.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m##  Chunk Parser Interface\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParserI\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/chunk/util.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy \u001b[38;5;28;01mas\u001b[39;00m _accuracy\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_tag\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m str2tuple\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tree\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/tag/__init__.py:72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaggerI\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m str2tuple, tuple2str, untag\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     73\u001b[0m     SequentialBackoffTagger,\n\u001b[1;32m     74\u001b[0m     ContextTagger,\n\u001b[1;32m     75\u001b[0m     DefaultTagger,\n\u001b[1;32m     76\u001b[0m     NgramTagger,\n\u001b[1;32m     77\u001b[0m     UnigramTagger,\n\u001b[1;32m     78\u001b[0m     BigramTagger,\n\u001b[1;32m     79\u001b[0m     TrigramTagger,\n\u001b[1;32m     80\u001b[0m     AffixTagger,\n\u001b[1;32m     81\u001b[0m     RegexpTagger,\n\u001b[1;32m     82\u001b[0m     ClassifierBasedTagger,\n\u001b[1;32m     83\u001b[0m     ClassifierBasedPOSTagger,\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrillTagger\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill_trainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrillTaggerTrainer\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/tag/sequential.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m jsontags\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeaturesetTaggerI, TaggerI\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/classify/__init__.py:97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpositivenaivebayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositiveNaiveBayesClassifier\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrte_classify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikitlearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SklearnClassifier\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msenna\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Senna\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextCat\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/classify/scikitlearn.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictionaryProbDist\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/sklearn/feature_extraction/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.feature_extraction` module deals with feature extraction\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mfrom raw data. It currently includes methods to extract features from text and\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mimages.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_vectorizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hash\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureHasher\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m img_to_graph, grid_to_graph\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m text\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/sklearn/feature_extraction/_hash.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IS_PYPY:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hashing_fast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transform \u001b[38;5;28;01mas\u001b[39;00m _hashing_transform\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_hashing_transform\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32msklearn/feature_extraction/_hashing_fast.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.feature_extraction._hashing_fast\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "ner_model = get_ner_model()\n",
    "get_location(\"У Дніпрі новий наступ\", ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "# Load spaCy model for Russian NER\n",
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "def normalize_words(words):\n",
    "    \"\"\"Normalize words using pymorphy2 (lemmatization).\"\"\"\n",
    "    morph = MorphAnalyzer()\n",
    "    return list({morph.parse(word)[0].normal_form for word in words})\n",
    "\n",
    "def get_name(mes):\n",
    "    \"\"\"Extracts person names (PER) from the text using spaCy.\"\"\"\n",
    "    doc = nlp(mes)\n",
    "    names = {ent.text for ent in doc.ents if ent.label_ == \"PER\"}\n",
    "    return normalize_words(names)\n",
    "\n",
    "def get_location(mes):\n",
    "    \"\"\"Extracts locations (LOC) from the text using spaCy.\"\"\"\n",
    "    doc = nlp(mes)\n",
    "    locations = {ent.text for ent in doc.ents if ent.label_ == \"LOC\"}\n",
    "    return normalize_words(locations)\n",
    "\n",
    "def get_weapons(mes, weapons_list_path=\"weapon.txt\"):\n",
    "    \"\"\"Checks for weapon names in the text using a predefined list from 'weapon.txt'.\"\"\"\n",
    "    try:\n",
    "        with open(weapons_list_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            weapons = {line.strip().lower() for line in file}\n",
    "        return [w for w in weapons if w in mes.lower()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: weapons.txt not found. Returning an empty list.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.8.0/ru_core_news_md-3.8.0-py3-none-any.whl (41.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-md==3.8.0)\n",
      "  Using cached pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (0.7.2)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
      "  Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Using cached pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
      "Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "Installing collected packages: pymorphy3-dicts-ru, pymorphy3, ru-core-news-md\n",
      "Successfully installed pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['милитополь']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_location(\"В городе Милитополе идут бои\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_bert(text, model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return torch.argmax(outputs.logits).item()\n",
    "\n",
    "def experience_xg_boost(text, xgb_model_path=\"/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/api/xgb_model.ubj\", bert_model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        transformer_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        transformer_model.to(device)\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = transformer_model(**inputs, output_hidden_states=True)\n",
    "            embeddings = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(xgb_model_path)\n",
    "        dmatrix = xgb.DMatrix(embeddings)\n",
    "        return int(xgb_model.predict(dmatrix)[0] > 0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experience_xg_boost: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_bert(\"завтра будет прорив , берите гранати\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in experience_xg_boost: [02:12:42] /Users/runner/work/xgboost/xgboost/src/common/json.cc:390: Expecting: \"L\", got: \"34 \", around character position: 2\n",
      "    {\"learner\"\n",
      "    ~^~~~~~~~~\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x00000001453a8428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x000000014549e180 xgboost::JsonReader::Error(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) const + 808\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000014549e714 xgboost::JsonReader::Expect(signed char, signed char) + 392\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x00000001454a0124 xgboost::UBJReader::DecodeStr() + 184\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x0000000145498e3c xgboost::UBJReader::ParseObject() + 144\n",
      "  [bt] (5) 6   libxgboost.dylib                    0x000000014549fad0 xgboost::UBJReader::Parse() + 708\n",
      "  [bt] (6) 7   libxgboost.dylib                    0x000000014549ecfc xgboost::Json::Load(xgboost::StringView, unsigned int) + 156\n",
      "  [bt] (7) 8   libxgboost.dylib                    0x000000014542bef4 XGBoosterLoadModel + 796\n",
      "  [bt] (8) 9   libffi.8.dylib                      0x000000010566c04c ffi_call_SYSV + 76\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_xg_boost(\"завтра будет прорив , берите гранати\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in experience_xg_boost: [02:13:57] /Users/runner/work/xgboost/xgboost/src/common/json.cc:390: Expecting: \"L\", got: \"34 \", around character position: 2\n",
      "    {\"learner\"\n",
      "    ~^~~~~~~~~\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x00000001453a8428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x000000014549e180 xgboost::JsonReader::Error(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) const + 808\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000014549e714 xgboost::JsonReader::Expect(signed char, signed char) + 392\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x00000001454a0124 xgboost::UBJReader::DecodeStr() + 184\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x0000000145498e3c xgboost::UBJReader::ParseObject() + 144\n",
      "  [bt] (5) 6   libxgboost.dylib                    0x000000014549fad0 xgboost::UBJReader::Parse() + 708\n",
      "  [bt] (6) 7   libxgboost.dylib                    0x000000014549ecfc xgboost::Json::Load(xgboost::StringView, unsigned int) + 156\n",
      "  [bt] (7) 8   libxgboost.dylib                    0x000000014542bef4 XGBoosterLoadModel + 796\n",
      "  [bt] (8) 9   libffi.8.dylib                      0x000000010566c04c ffi_call_SYSV + 76\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_xg_boost(\"у нас новие гранати ПМ16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers sentencepiece\n",
    "# !pip install transformers torch\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer, T5ForConditionalGeneration, MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "def gen_o(text: str, model_name: str = \"cointegrated/rut5-base-absum\"):\n",
    "    \"\"\"Генерация спостереження (Observation)\"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    prompt = \"У чем проблема: \" + text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    output_ids = model.generate(input_ids, num_beams=5, early_stopping=True, max_length=256)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def gen_d(text, max_length=512, min_length=1, lang_code=\"ru_RU\"):\n",
    "    \"\"\"Генерация обоснования (Discussion)\"\"\"\n",
    "    model_name = \"facebook/mbart-large-50\"\n",
    "    tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Переконаємося, що lang_code існує\n",
    "    if lang_code not in tokenizer.lang_code_to_id:\n",
    "        print(f\"Warning: lang_code {lang_code} not found! Using 'ru'.\")\n",
    "        lang_code = \"ru\"\n",
    "\n",
    "    prompt = \"Объясните, почему это произошло: \" + text  # Повернув промпт!\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        inputs, max_length=max_length, min_length=min_length, length_penalty=2.0,\n",
    "        num_beams=4, early_stopping=True, forced_bos_token_id=tokenizer.lang_code_to_id[lang_code]\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def gen_c(text: str, model_name: str = \"IlyaGusev/rut5_base_sum_gazeta\"):\n",
    "    \"\"\"Генерация вывода (Conclusion)\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    prompt = \"Сформулируйте краткий и четкий вывод на основе текста: \" + text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    output_ids = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def gen_r(text: str, model_name: str = \"cointegrated/rut5-base-absum\"):\n",
    "    \"\"\"Генерация рекомендации (Recommendation)\"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    prompt = \"Опишите, что произошло и что нужно делать: \" + text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    output_ids = model.generate(input_ids, num_beams=5, early_stopping=True, max_length=256)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Посмотрите, что происходит. Посмотрите, что происходит.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "\n",
    "gen_r(\"войска наступают\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error receiving messages: 'coroutine' object has no attribute 'get_chat'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/srzxpdh14hlgl_4b4d260fm40000gn/T/ipykernel_51176/1212648523.py:36: RuntimeWarning: coroutine 'Start.start' was never awaited\n",
      "  messages, dates, channels, ids = fetch_messages(start_date, end_date, \"ДРОННИЦА\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from pyrogram import Client\n",
    "import asyncio\n",
    "\n",
    "def fetch_messages(start_date, end_date, channel_name):\n",
    "    if channel_name == \"Вертолатте\":\n",
    "        channel = \"@vertolatte\"\n",
    "    elif channel_name == \"ДРОННИЦА\":\n",
    "        channel = \"@dronnitsa\"\n",
    "    else:\n",
    "        channel = \"@donbassrussiazvo\"\n",
    "    \n",
    "    messages, dates, channels, ids = [], [], [], []\n",
    "    \n",
    "    with Client(\"military_bot\", API_ID, API_HASH) as app:\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            chat = loop.run_until_complete(app.get_chat(channel))\n",
    "            \n",
    "            for message in loop.run_until_complete(app.get_chat_history(chat.id)):\n",
    "                if not message.date or message.date < start_date:\n",
    "                    break\n",
    "                \n",
    "                if start_date <= message.date <= end_date:\n",
    "                    message_text = message.text if message.text else message.caption\n",
    "                    if message_text:\n",
    "                        messages.append(message_text)\n",
    "                        print(message_text)\n",
    "                        dates.append(message.date.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])\n",
    "                        channels.append(channel)\n",
    "                        ids.append(message.id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error receiving messages: {e}\")\n",
    "    \n",
    "    return messages, dates, channels, ids\n",
    "\n",
    "messages, dates, channels, ids = fetch_messages(start_date, end_date, \"ДРОННИЦА\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'prediction': 0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL вашого Flask сервера (локально на порті 5003)\n",
    "API_URL = \"http://127.0.0.1:5003/predict/bert\"\n",
    "\n",
    "# Тестовий текст для класифікації\n",
    "test_data = {\"text\": \"Це тестовий текст для моделі BERT\"}\n",
    "\n",
    "# Відправлення POST-запиту\n",
    "response = requests.post(API_URL, json=test_data)\n",
    "\n",
    "# Виведення результату\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load spaCy model for Russian NER\n",
    "# nlp = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "# def normalize_words(words):\n",
    "#     \"\"\"Normalize words using pymorphy2 (lemmatization).\"\"\"\n",
    "#     morph = MorphAnalyzer()\n",
    "#     return list({morph.parse(word)[0].normal_form for word in words})\n",
    "\n",
    "# def get_name(mes):\n",
    "#     \"\"\"Extracts person names (PER) from the text using spaCy.\"\"\"\n",
    "#     doc = nlp(mes)\n",
    "#     names = {ent.text for ent in doc.ents if ent.label_ == \"PER\"}\n",
    "#     return normalize_words(names)\n",
    "\n",
    "# def get_location(mes):\n",
    "#     \"\"\"Extracts locations (LOC) from the text using spaCy.\"\"\"\n",
    "#     doc = nlp(mes)\n",
    "#     locations = {ent.text for ent in doc.ents if ent.label_ == \"LOC\"}\n",
    "#     return normalize_words(locations)\n",
    "\n",
    "# def get_weapons(mes, weapons_list_path=\"weapon.txt\"):\n",
    "#     \"\"\"Checks for weapon names in the text using a predefined list from 'weapon.txt'.\"\"\"\n",
    "#     try:\n",
    "#         with open(weapons_list_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#             weapons = {line.strip().lower() for line in file}\n",
    "#         return [w for w in weapons if w in mes.lower()]\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"Warning: weapons.txt not found. Returning an empty list.\")\n",
    "#         return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28.0\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from openai==0.28.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from openai==0.28.0) (4.64.1)\n",
      "Collecting aiohttp (from openai==0.28.0)\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.28.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.28.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests>=2.20->openai==0.28.0) (2024.12.14)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->openai==0.28.0)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai==0.28.0)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->openai==0.28.0)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->openai==0.28.0)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai==0.28.0)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai==0.28.0)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->openai==0.28.0)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->openai==0.28.0)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28.0) (4.12.2)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Downloading aiohttp-3.11.12-cp310-cp310-macosx_11_0_arm64.whl (455 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "Downloading multidict-6.1.0-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-macosx_11_0_arm64.whl (92 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, attrs, async-timeout, aiohappyeyeballs, yarl, aiosignal, aiohttp, openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.61.1\n",
      "    Uninstalling openai-1.61.1:\n",
      "      Successfully uninstalled openai-1.61.1\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.1.0 frozenlist-1.5.0 multidict-6.1.0 openai-0.28.0 propcache-0.2.1 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
