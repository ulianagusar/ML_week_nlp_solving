{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unique_ids': [1, 2]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:5000/remove_duplicates?day_range=1\"\n",
    "\n",
    "data = {\n",
    "    \"Message\": [\"–ü—Ä–∏–≤—ñ—Ç\", \"–Ø–∫ —Å–ø—Ä–∞–≤–∏?\", \"–ü—Ä–∏–≤—ñ—Ç\"],\n",
    "    \"MessageDate\": [\"2029-01-28 19:28:10.123\", \"2029-01-29 10:00:00.000\", \"2029-01-28 19:28:10.123\"],\n",
    "    \"TelegramPostInfoID\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'message': '–ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ –æ—Ç—Ä–∏–º–∞–Ω—ñ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "url = \"http://127.0.0.1:5001/api/fetch_posts\"  \n",
    "payload = {\n",
    "    \"channel\": \"news_channel\",\n",
    "    \"start_date\": \"2024-02-01\",\n",
    "    \"end_date\": \"2024-02-05\",\n",
    "    \"model\": \"Bert\"  \n",
    "}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/backend/database/database.db\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/backend/database/database.db\"\n",
    "print(DB_PATH)\n",
    "import sqlite3\n",
    "def get_db_connection():\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)  \n",
    "    conn.row_factory = sqlite3.Row \n",
    "    return conn\n",
    "\n",
    "\n",
    "def fetch_table_contents(table_name):\n",
    "\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "\n",
    "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        if rows:\n",
    "\n",
    "            for row in rows:\n",
    "                print(dict(row)) \n",
    "        else:\n",
    "            print(f\" {table_name} empty.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "def show_all_tables():\n",
    "    fetch_table_contents(\"TelegramPostInfo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå –í–º—ñ—Å—Ç —Ç–∞–±–ª–∏—Ü—ñ TelegramPostInfo:\n",
      "{'MessageID': 8420, 'Message': '–í –ø–∞—Ä–∫–µ –≤–∏–Ω—Ç–æ–∫—Ä—ã–ª—ã—Ö –º–∞—à–∏–Ω 2-–≥–æ –ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –∞–≤–∏–∞–æ—Ç—Ä—è–¥–∞ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ. –°–æ–≤–µ—Ä—à–∏–≤ –ø—Ä—è–º–æ–π –ø–µ—Ä–µ–ª—ë—Ç –∏–∑ –ö–∞–∑–∞–Ω–∏ –∑–∞ –ø—è—Ç—å —á–∞—Å–æ–≤ –≤ –í–∞—Å—å–∫–æ–≤–æ –ø—Ä–∏–±—ã–ª –Ω–æ–≤—ã–π –ú–∏-8–ú–¢–í-1. –î–∞–Ω–Ω—ã–π –≤–µ—Ä—Ç–æ–ª–µ—Ç –ø–ª–∞–Ω–∏—Ä—É—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–∑–∫–∏ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤ —Ç—Ä—É–¥–Ω–æ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π, –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–∞–Ω–∏—Ç–∞—Ä–Ω–æ–π –∞–≤–∏–∞—Ü–∏–∏ –∏ –¥–æ—Å—Ç–∞–≤–∫–∏ –≥—Ä—É–∑–æ–≤.\\n\\n#–≤–µ—Ä—Ç–æ–Ω–æ–≤–æ—Å—Ç–∏', 'Channel': '@vertolatte', 'MessageDate': '2025-01-02 14:17:49.000', 'Name': '', 'Location': '–≤–∞—Å—å–∫–æ–≤—ã–π, –∫–∞–∑–∞–Ω—å', 'Weapons': '—Ç–æ—Ä', 'Observation': 'o', 'Discussion': 'd', 'Conclusion': 's', 'Recommendation': 'r'}\n"
     ]
    }
   ],
   "source": [
    "show_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Channel': '@vertolatte', 'Message': '–í –ø–∞—Ä–∫–µ –≤–∏–Ω—Ç–æ–∫—Ä—ã–ª—ã—Ö –º–∞—à–∏–Ω 2-–≥–æ –ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –∞–≤–∏–∞–æ—Ç—Ä—è–¥–∞ –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ. –°–æ–≤–µ—Ä—à–∏–≤ –ø—Ä—è–º–æ–π –ø–µ—Ä–µ–ª—ë—Ç –∏–∑ –ö–∞–∑–∞–Ω–∏ –∑–∞ –ø—è—Ç—å —á–∞—Å–æ–≤ –≤ –í–∞—Å—å–∫–æ–≤–æ –ø—Ä–∏–±—ã–ª –Ω–æ–≤—ã–π –ú–∏-8–ú–¢–í-1. –î–∞–Ω–Ω—ã–π –≤–µ—Ä—Ç–æ–ª–µ—Ç –ø–ª–∞–Ω–∏—Ä—É—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–∑–∫–∏ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤ —Ç—Ä—É–¥–Ω–æ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π, –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–∞–Ω–∏—Ç–∞—Ä–Ω–æ–π –∞–≤–∏–∞—Ü–∏–∏ –∏ –¥–æ—Å—Ç–∞–≤–∫–∏ –≥—Ä—É–∑–æ–≤.\\n\\n#–≤–µ—Ä—Ç–æ–Ω–æ–≤–æ—Å—Ç–∏', 'MessageDate': '2025-01-02 14:17:49.000', 'TelegramPostInfoID': 8420}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "API_URL = \"http://127.0.0.1:5001/api/posts\"  \n",
    "\n",
    "def test_get_posts():\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_URL)  \n",
    "        if response.status_code == 200:\n",
    "            posts = response.json()\n",
    "\n",
    "            for post in posts:\n",
    "                print(post)\n",
    "        else:\n",
    "            print(f\" {response.status_code}, response: {response.text}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" {e}\")\n",
    "test_get_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeppavlov\n",
      "  Downloading deeppavlov-1.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting fastapi<=0.89.1,>=0.47.0 (from deeppavlov)\n",
      "  Downloading fastapi-0.89.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting filelock<3.10.0,>=3.0.0 (from deeppavlov)\n",
      "  Downloading filelock-3.9.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting nltk<3.10.0,>=3.2.4 (from deeppavlov)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy<1.24 (from deeppavlov)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting pandas<1.6.0,>=1.0.0 (from deeppavlov)\n",
      "  Downloading pandas-1.5.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting prometheus-client<=1.16.0,>=0.13.0 (from deeppavlov)\n",
      "  Downloading prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pydantic<2 (from deeppavlov)\n",
      "  Downloading pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl.metadata (153 kB)\n",
      "Collecting pybind11==2.10.3 (from deeppavlov)\n",
      "  Downloading pybind11-2.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.19.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from deeppavlov) (2.32.3)\n",
      "Collecting tqdm<4.65.0,>=4.42.0 (from deeppavlov)\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "Collecting uvicorn<0.19.0,>=0.13.0 (from deeppavlov)\n",
      "  Downloading uvicorn-0.18.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: wheel in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from deeppavlov) (0.45.1)\n",
      "Collecting scikit-learn<1.1.0,>=0.24 (from deeppavlov)\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (10 kB)\n",
      "Collecting scipy==1.10.0 (from deeppavlov)\n",
      "  Downloading scipy-1.10.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (53 kB)\n",
      "Collecting starlette==0.22.0 (from fastapi<=0.89.1,>=0.47.0->deeppavlov)\n",
      "  Downloading starlette-0.22.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (4.8.0)\n",
      "Requirement already satisfied: click in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2024.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pydantic<2->deeppavlov) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2024.12.14)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from scikit-learn<1.1.0,>=0.24->deeppavlov) (3.5.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from uvicorn<0.19.0,>=0.13.0->deeppavlov) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<1.6.0,>=1.0.0->deeppavlov) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.3.1)\n",
      "Downloading deeppavlov-1.7.0-py3-none-any.whl (492 kB)\n",
      "Downloading pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
      "Downloading scipy-1.10.0-cp310-cp310-macosx_12_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.89.1-py3-none-any.whl (55 kB)\n",
      "Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
      "Downloading filelock-3.9.1-py3-none-any.whl (9.7 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-macosx_11_0_arm64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "Downloading pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.0.2-cp310-cp310-macosx_12_0_arm64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
      "Installing collected packages: uvicorn, tqdm, pydantic, pybind11, prometheus-client, numpy, filelock, starlette, scipy, pandas, nltk, scikit-learn, fastapi, deeppavlov\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.6\n",
      "    Uninstalling pydantic-2.10.6:\n",
      "      Successfully uninstalled pydantic-2.10.6\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.2\n",
      "    Uninstalling numpy-2.2.2:\n",
      "      Successfully uninstalled numpy-2.2.2\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.17.0\n",
      "    Uninstalling filelock-3.17.0:\n",
      "      Successfully uninstalled filelock-3.17.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.1\n",
      "    Uninstalling scipy-1.15.1:\n",
      "      Successfully uninstalled scipy-1.15.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "faiss-cpu 1.9.0.post1 requires numpy<3.0,>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed deeppavlov-1.7.0 fastapi-0.89.1 filelock-3.9.1 nltk-3.9.1 numpy-1.23.5 pandas-1.5.3 prometheus-client-0.21.1 pybind11-2.10.3 pydantic-1.10.21 scikit-learn-1.0.2 scipy-1.10.0 starlette-0.22.0 tqdm-4.64.1 uvicorn-0.18.3\n"
     ]
    }
   ],
   "source": [
    "# experience.py\n",
    "#!pip install pymorphy2 pymorphy2-dicts-ru\n",
    "!pip install deeppavlov\n",
    "\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def experience_bert(text, model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return torch.argmax(outputs.logits).item()\n",
    "\n",
    "def experience_xg_boost(text, xgb_model_path=\"xgb_model.ubj\", bert_model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        transformer_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        transformer_model.to(device)\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = transformer_model(**inputs, output_hidden_states=True)\n",
    "            embeddings = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(xgb_model_path)\n",
    "        dmatrix = xgb.DMatrix(embeddings)\n",
    "        return int(xgb_model.predict(dmatrix)[0] > 0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experience_xg_boost: {e}\")\n",
    "        return 0\n",
    "\n",
    "# ner.py\n",
    "from deeppavlov import build_model, configs\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def get_ner_model():\n",
    "    return build_model(configs.ner.ner_rus_bert, download=True)\n",
    "\n",
    "def normalize_words(words):\n",
    "    morph = MorphAnalyzer()\n",
    "    return list({morph.parse(word)[0].normal_form for word in words})\n",
    "\n",
    "def get_name(mes, ner_model):\n",
    "    ner_results = ner_model([mes])\n",
    "    names = {word for word, label in zip(ner_results[0][0], ner_results[1][0]) if label in [\"B-PER\", \"I-PER\"]}\n",
    "    return normalize_words(names)\n",
    "\n",
    "def get_location(mes, ner_model):\n",
    "    ner_results = ner_model([mes])\n",
    "    locations = {word for word, label in zip(ner_results[0][0], ner_results[1][0]) if label in [\"B-LOC\", \"I-LOC\"]}\n",
    "    return normalize_words(locations)\n",
    "\n",
    "def get_weapons(mes, weapons_list_path=\"weapon.txt\"):\n",
    "    try:\n",
    "        with open(weapons_list_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            weapons = {line.strip().lower() for line in file}\n",
    "        return [w for w in weapons if w in mes.lower()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: weapons.txt not found. Returning empty list.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deeppavlov 1.7.0 requires numpy<1.24, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 01:43:07.349 INFO in 'deeppavlov.core.data.utils'['utils'] at line 97: Downloading from http://files.deeppavlov.ai/v1/ner/ner_rus_bert_torch_new.tar.gz to /Users/ulanagusar/.deeppavlov/models/ner_rus_bert_torch_new.tar.gz\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.44G/1.44G [03:10<00:00, 7.57MB/s]\n",
      "2025-02-08 01:46:17.976 INFO in 'deeppavlov.core.data.utils'['utils'] at line 284: Extracting /Users/ulanagusar/.deeppavlov/models/ner_rus_bert_torch_new.tar.gz archive into /Users/ulanagusar/.deeppavlov/models/ner_rus_bert_torch\n",
      "/Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages/sklearn/utils/__init__.py:108: FutureWarning: IS_PYPY is deprecated and will be removed in 1.7.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_ner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_location(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–£ –î–Ω—ñ–ø—Ä—ñ –Ω–æ–≤–∏–π –Ω–∞—Å—Ç—É–ø\u001b[39m\u001b[38;5;124m\"\u001b[39m, ner_model)\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mget_ner_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_ner_model\u001b[39m():\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner_rus_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/commands/infer.py:55\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m parameter for the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m component, so \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m will not be renewed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m, component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m))))\n\u001b[0;32m---> 55\u001b[0m component \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m component_config:\n\u001b[1;32m     58\u001b[0m     model\u001b[38;5;241m.\u001b[39m_components_dict[component_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m component\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/common/params.py:92\u001b[0m, in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(e)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m---> 92\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# find the submodels params recursively\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     config_params \u001b[38;5;241m=\u001b[39m {k: _init_param(v, mode) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config_params\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/common/registry.py:74\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not registered.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_from_str(name)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcls_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_REGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/core/common/registry.py:42\u001b[0m, in \u001b[0;36mcls_from_str\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected class description in a `module.submodules:ClassName` form, but got `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m                       \u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m, cls_name)\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/deeppavlov/models/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeeppavlov\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RedirectedPrints\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDP_SKIP_NLTK_DOWNLOAD\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/__init__.py:146\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/chunk/__init__.py:155\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamed_entity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/chunk/api.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m##  Chunk Parser Interface\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParserI\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/chunk/util.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy \u001b[38;5;28;01mas\u001b[39;00m _accuracy\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_tag\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m str2tuple\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tree\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/tag/__init__.py:72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaggerI\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m str2tuple, tuple2str, untag\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     73\u001b[0m     SequentialBackoffTagger,\n\u001b[1;32m     74\u001b[0m     ContextTagger,\n\u001b[1;32m     75\u001b[0m     DefaultTagger,\n\u001b[1;32m     76\u001b[0m     NgramTagger,\n\u001b[1;32m     77\u001b[0m     UnigramTagger,\n\u001b[1;32m     78\u001b[0m     BigramTagger,\n\u001b[1;32m     79\u001b[0m     TrigramTagger,\n\u001b[1;32m     80\u001b[0m     AffixTagger,\n\u001b[1;32m     81\u001b[0m     RegexpTagger,\n\u001b[1;32m     82\u001b[0m     ClassifierBasedTagger,\n\u001b[1;32m     83\u001b[0m     ClassifierBasedPOSTagger,\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrillTagger\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill_trainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BrillTaggerTrainer\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/tag/sequential.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m jsontags\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeaturesetTaggerI, TaggerI\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/classify/__init__.py:97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpositivenaivebayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PositiveNaiveBayesClassifier\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrte_classify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikitlearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SklearnClassifier\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msenna\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Senna\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextCat\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/nltk/classify/scikitlearn.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictionaryProbDist\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/sklearn/feature_extraction/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.feature_extraction` module deals with feature extraction\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mfrom raw data. It currently includes methods to extract features from text and\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mimages.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_vectorizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hash\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureHasher\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m img_to_graph, grid_to_graph\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m text\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/sklearn/feature_extraction/_hash.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IS_PYPY:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hashing_fast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transform \u001b[38;5;28;01mas\u001b[39;00m _hashing_transform\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_hashing_transform\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32msklearn/feature_extraction/_hashing_fast.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.feature_extraction._hashing_fast\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "ner_model = get_ner_model()\n",
    "get_location(\"–£ –î–Ω—ñ–ø—Ä—ñ –Ω–æ–≤–∏–π –Ω–∞—Å—Ç—É–ø\", ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "# Load spaCy model for Russian NER\n",
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "def normalize_words(words):\n",
    "    \"\"\"Normalize words using pymorphy2 (lemmatization).\"\"\"\n",
    "    morph = MorphAnalyzer()\n",
    "    return list({morph.parse(word)[0].normal_form for word in words})\n",
    "\n",
    "def get_name(mes):\n",
    "    \"\"\"Extracts person names (PER) from the text using spaCy.\"\"\"\n",
    "    doc = nlp(mes)\n",
    "    names = {ent.text for ent in doc.ents if ent.label_ == \"PER\"}\n",
    "    return normalize_words(names)\n",
    "\n",
    "def get_location(mes):\n",
    "    \"\"\"Extracts locations (LOC) from the text using spaCy.\"\"\"\n",
    "    doc = nlp(mes)\n",
    "    locations = {ent.text for ent in doc.ents if ent.label_ == \"LOC\"}\n",
    "    return normalize_words(locations)\n",
    "\n",
    "def get_weapons(mes, weapons_list_path=\"weapon.txt\"):\n",
    "    \"\"\"Checks for weapon names in the text using a predefined list from 'weapon.txt'.\"\"\"\n",
    "    try:\n",
    "        with open(weapons_list_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            weapons = {line.strip().lower() for line in file}\n",
    "        return [w for w in weapons if w in mes.lower()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: weapons.txt not found. Returning an empty list.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.8.0/ru_core_news_md-3.8.0-py3-none-any.whl (41.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.9/41.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-md==3.8.0)\n",
      "  Using cached pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (0.7.2)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
      "  Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Using cached pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
      "Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "Installing collected packages: pymorphy3-dicts-ru, pymorphy3, ru-core-news-md\n",
      "Successfully installed pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-md-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–º–∏–ª–∏—Ç–æ–ø–æ–ª—å']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_location(\"–í –≥–æ—Ä–æ–¥–µ –ú–∏–ª–∏—Ç–æ–ø–æ–ª–µ –∏–¥—É—Ç –±–æ–∏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_bert(text, model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        return torch.argmax(outputs.logits).item()\n",
    "\n",
    "def experience_xg_boost(text, xgb_model_path=\"/Users/ulanagusar/Desktop/ML_week/ML_week_nlp_solving/api/xgb_model.ubj\", bert_model_name=\"bodomerka/Milytary_exp_class_classification_sber_ai_based\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        transformer_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        transformer_model.to(device)\n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = transformer_model(**inputs, output_hidden_states=True)\n",
    "            embeddings = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        xgb_model = xgb.Booster()\n",
    "        xgb_model.load_model(xgb_model_path)\n",
    "        dmatrix = xgb.DMatrix(embeddings)\n",
    "        return int(xgb_model.predict(dmatrix)[0] > 0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in experience_xg_boost: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_bert(\"–∑–∞–≤—Ç—Ä–∞ –±—É–¥–µ—Ç –ø—Ä–æ—Ä–∏–≤ , –±–µ—Ä–∏—Ç–µ –≥—Ä–∞–Ω–∞—Ç–∏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in experience_xg_boost: [02:12:42] /Users/runner/work/xgboost/xgboost/src/common/json.cc:390: Expecting: \"L\", got: \"34 \", around character position: 2\n",
      "    {\"learner\"\n",
      "    ~^~~~~~~~~\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x00000001453a8428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x000000014549e180 xgboost::JsonReader::Error(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) const + 808\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000014549e714 xgboost::JsonReader::Expect(signed char, signed char) + 392\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x00000001454a0124 xgboost::UBJReader::DecodeStr() + 184\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x0000000145498e3c xgboost::UBJReader::ParseObject() + 144\n",
      "  [bt] (5) 6   libxgboost.dylib                    0x000000014549fad0 xgboost::UBJReader::Parse() + 708\n",
      "  [bt] (6) 7   libxgboost.dylib                    0x000000014549ecfc xgboost::Json::Load(xgboost::StringView, unsigned int) + 156\n",
      "  [bt] (7) 8   libxgboost.dylib                    0x000000014542bef4 XGBoosterLoadModel + 796\n",
      "  [bt] (8) 9   libffi.8.dylib                      0x000000010566c04c ffi_call_SYSV + 76\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_xg_boost(\"–∑–∞–≤—Ç—Ä–∞ –±—É–¥–µ—Ç –ø—Ä–æ—Ä–∏–≤ , –±–µ—Ä–∏—Ç–µ –≥—Ä–∞–Ω–∞—Ç–∏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in experience_xg_boost: [02:13:57] /Users/runner/work/xgboost/xgboost/src/common/json.cc:390: Expecting: \"L\", got: \"34 \", around character position: 2\n",
      "    {\"learner\"\n",
      "    ~^~~~~~~~~\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x00000001453a8428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x000000014549e180 xgboost::JsonReader::Error(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) const + 808\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000014549e714 xgboost::JsonReader::Expect(signed char, signed char) + 392\n",
      "  [bt] (3) 4   libxgboost.dylib                    0x00000001454a0124 xgboost::UBJReader::DecodeStr() + 184\n",
      "  [bt] (4) 5   libxgboost.dylib                    0x0000000145498e3c xgboost::UBJReader::ParseObject() + 144\n",
      "  [bt] (5) 6   libxgboost.dylib                    0x000000014549fad0 xgboost::UBJReader::Parse() + 708\n",
      "  [bt] (6) 7   libxgboost.dylib                    0x000000014549ecfc xgboost::Json::Load(xgboost::StringView, unsigned int) + 156\n",
      "  [bt] (7) 8   libxgboost.dylib                    0x000000014542bef4 XGBoosterLoadModel + 796\n",
      "  [bt] (8) 9   libffi.8.dylib                      0x000000010566c04c ffi_call_SYSV + 76\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_xg_boost(\"—É –Ω–∞—Å –Ω–æ–≤–∏–µ –≥—Ä–∞–Ω–∞—Ç–∏ –ü–ú16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (4.48.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (4.48.1)\n",
      "Requirement already satisfied: torch in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulanagusar/Desktop/ML_week/.conda/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece\n",
    "!pip install transformers torch\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer, T5ForConditionalGeneration, MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "def gen_o(text: str, model_name: str = \"cointegrated/rut5-base-absum\"):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è (Observation)\"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    prompt = \"–£ —á–µ–º –ø—Ä–æ–±–ª–µ–º–∞: \" + text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    output_ids = model.generate(input_ids, num_beams=5, early_stopping=True, max_length=256)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def gen_d(text, max_length=512, min_length=50, lang_code=\"ru_RU\"):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è (Discussion)\"\"\"\n",
    "    model_name = \"facebook/mbart-large-50\"\n",
    "    tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # –ü–µ—Ä–µ–∫–æ–Ω–∞—î–º–æ—Å—è, —â–æ lang_code —ñ—Å–Ω—É—î\n",
    "    if lang_code not in tokenizer.lang_code_to_id:\n",
    "        print(f\"Warning: lang_code {lang_code} not found! Using 'ru'.\")\n",
    "        lang_code = \"ru\"\n",
    "\n",
    "    prompt = \"–û–±—ä—è—Å–Ω–∏—Ç–µ, –ø–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ: \" + text  # –ü–æ–≤–µ—Ä–Ω—É–≤ –ø—Ä–æ–º–ø—Ç!\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    output_ids = model.generate(\n",
    "        inputs, max_length=max_length, min_length=min_length, length_penalty=2.0,\n",
    "        num_beams=4, early_stopping=True, forced_bos_token_id=tokenizer.lang_code_to_id[lang_code]\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def gen_c(text: str, model_name: str = \"IlyaGusev/rut5_base_sum_gazeta\"):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞ (Conclusion)\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    prompt = \"–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∫—Ä–∞—Ç–∫–∏–π –∏ —á–µ—Ç–∫–∏–π –≤—ã–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞: \" + text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    output_ids = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def gen_r(text: str, model_name: str = \"cointegrated/rut5-base-absum\"):\n",
    "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (Recommendation)\"\"\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    prompt = \"–û–ø–∏—à–∏—Ç–µ, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ —á—Ç–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å: \" + text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    output_ids = model.generate(input_ids, num_beams=5, early_stopping=True, max_length=256)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgen_r\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m–≤–æ–π—Å–∫–∞ –Ω–∞—Å—Ç—É–ø–∞—é—Ç\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 46\u001b[0m, in \u001b[0;36mgen_r\u001b[0;34m(text, model_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgen_r\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcointegrated/rut5-base-absum\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (Recommendation)\"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m     47\u001b[0m     model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     48\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–û–ø–∏—à–∏—Ç–µ, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ —á—Ç–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ML_week/.conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "gen_r(\"–≤–æ–π—Å–∫–∞ –Ω–∞—Å—Ç—É–ø–∞—é—Ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error receiving messages: 'coroutine' object has no attribute 'get_chat'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/srzxpdh14hlgl_4b4d260fm40000gn/T/ipykernel_51176/1212648523.py:36: RuntimeWarning: coroutine 'Start.start' was never awaited\n",
      "  messages, dates, channels, ids = fetch_messages(start_date, end_date, \"–î–†–û–ù–ù–ò–¶–ê\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from pyrogram import Client\n",
    "import asyncio\n",
    "\n",
    "def fetch_messages(start_date, end_date, channel_name):\n",
    "    if channel_name == \"–í–µ—Ä—Ç–æ–ª–∞—Ç—Ç–µ\":\n",
    "        channel = \"@vertolatte\"\n",
    "    elif channel_name == \"–î–†–û–ù–ù–ò–¶–ê\":\n",
    "        channel = \"@dronnitsa\"\n",
    "    else:\n",
    "        channel = \"@donbassrussiazvo\"\n",
    "    \n",
    "    messages, dates, channels, ids = [], [], [], []\n",
    "    \n",
    "    with Client(\"military_bot\", API_ID, API_HASH) as app:\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            chat = loop.run_until_complete(app.get_chat(channel))\n",
    "            \n",
    "            for message in loop.run_until_complete(app.get_chat_history(chat.id)):\n",
    "                if not message.date or message.date < start_date:\n",
    "                    break\n",
    "                \n",
    "                if start_date <= message.date <= end_date:\n",
    "                    message_text = message.text if message.text else message.caption\n",
    "                    if message_text:\n",
    "                        messages.append(message_text)\n",
    "                        print(message_text)\n",
    "                        dates.append(message.date.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])\n",
    "                        channels.append(channel)\n",
    "                        ids.append(message.id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error receiving messages: {e}\")\n",
    "    \n",
    "    return messages, dates, channels, ids\n",
    "\n",
    "messages, dates, channels, ids = fetch_messages(start_date, end_date, \"–î–†–û–ù–ù–ò–¶–ê\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'prediction': 0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL –≤–∞—à–æ–≥–æ Flask —Å–µ—Ä–≤–µ—Ä–∞ (–ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ –ø–æ—Ä—Ç—ñ 5003)\n",
    "API_URL = \"http://127.0.0.1:5003/predict/bert\"\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó\n",
    "test_data = {\"text\": \"–¶–µ —Ç–µ—Å—Ç–æ–≤–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª—ñ BERT\"}\n",
    "\n",
    "# –í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–Ω—è POST-–∑–∞–ø–∏—Ç—É\n",
    "response = requests.post(API_URL, json=test_data)\n",
    "\n",
    "# –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load spaCy model for Russian NER\n",
    "# nlp = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "# def normalize_words(words):\n",
    "#     \"\"\"Normalize words using pymorphy2 (lemmatization).\"\"\"\n",
    "#     morph = MorphAnalyzer()\n",
    "#     return list({morph.parse(word)[0].normal_form for word in words})\n",
    "\n",
    "# def get_name(mes):\n",
    "#     \"\"\"Extracts person names (PER) from the text using spaCy.\"\"\"\n",
    "#     doc = nlp(mes)\n",
    "#     names = {ent.text for ent in doc.ents if ent.label_ == \"PER\"}\n",
    "#     return normalize_words(names)\n",
    "\n",
    "# def get_location(mes):\n",
    "#     \"\"\"Extracts locations (LOC) from the text using spaCy.\"\"\"\n",
    "#     doc = nlp(mes)\n",
    "#     locations = {ent.text for ent in doc.ents if ent.label_ == \"LOC\"}\n",
    "#     return normalize_words(locations)\n",
    "\n",
    "# def get_weapons(mes, weapons_list_path=\"weapon.txt\"):\n",
    "#     \"\"\"Checks for weapon names in the text using a predefined list from 'weapon.txt'.\"\"\"\n",
    "#     try:\n",
    "#         with open(weapons_list_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#             weapons = {line.strip().lower() for line in file}\n",
    "#         return [w for w in weapons if w in mes.lower()]\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"Warning: weapons.txt not found. Returning an empty list.\")\n",
    "#         return []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
